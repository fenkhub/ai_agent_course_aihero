{
 "cells": [
  {
   "cell_type": "raw",
   "id": "482f3f2f-0ac0-4048-a561-1b4e3cf2826e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Getting Started with AI\"\n",
    "author: \"John Doe\"\n",
    "date: \"2024-01-15\"\n",
    "tags: [\"ai\", \"machine-learning\", \"tutorial\"]\n",
    "difficulty: \"beginner\"\n",
    "---\n",
    "\n",
    "# Getting Started with AI\n",
    "\n",
    "This is the main content of the document written in **Markdown**.\n",
    "\n",
    "You can include code blocks, links, and other formatting here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2780fa80-db34-4bd2-9119-d21215eec975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87eebc54-eed3-4a17-8876-999e512112f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 4\n",
      "Evidently documents: 93\n"
     ]
    }
   ],
   "source": [
    "brdt_mcp = read_repo_data('brightdata', 'brightdata-mcp')\n",
    "ai_docs = read_repo_data('patchy631', 'ai-engineering-hub')\n",
    "\n",
    "print(f\"FAQ documents: {len(brdt_mcp)}\")\n",
    "print(f\"Evidently documents: {len(ai_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16c0ca41-ee4b-43e3-8028-ffc1336cbc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "groq_client = Groq()  # Or use os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()\n",
    "\n",
    "def llm(document, model='llama-3.1-8b-instant'):\n",
    "    prompt = prompt_template.format(document=document)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that organizes documents for Q&A systems.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20184a68-1cc8-4975-987f-68c88aaeb6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "groq_client = Groq()  # api_key=\"your-api-key Or use environment variable\n",
    "\n",
    "def llm(prompt, model='llama-3.1-8b-instant'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0551eadc-74cf-4e48-80e3-c223489ee3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88d3c783-b01b-4b91-9a6f-80b51614c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76df350b-d806-4115-bf97-96071c616242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4128db9586647789936d1e7941390be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(brdt_mcp):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd39a9-eddf-40d6-a25e-347995a5465c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
