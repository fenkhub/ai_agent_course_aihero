{
 "cells": [
  {
   "cell_type": "raw",
   "id": "99d1e313-6d96-4e18-bbff-806ee84bba49",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Getting Started with AI\"\n",
    "author: \"John Doe\"\n",
    "date: \"2024-01-15\"\n",
    "tags: [\"ai\", \"machine-learning\", \"tutorial\"]\n",
    "difficulty: \"beginner\"\n",
    "---\n",
    "\n",
    "# Getting Started with AI\n",
    "\n",
    "This is the main content of the document written in **Markdown**.\n",
    "\n",
    "You can include code blocks, links, and other formatting here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e58dd8-25ed-48b6-abb1-e09a77816fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d4d351f-9fab-4cae-9635-a9d39238e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "569150eb-20c2-40ed-ae9f-9ac3809db6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeload.github.com/DataTalksClub/faq/zip/refs/heads/main'\n",
    "resp = requests.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "184c6c07-f08a-45e7-8d2b-5d26bd117899",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_data = []\n",
    "\n",
    "# Create a ZipFile object from the downloaded content\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    # Only process markdown files\n",
    "    if not filename.endswith('.md'):\n",
    "        continue\n",
    "\n",
    "    # Read and parse each file\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read()\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "692de8f7-9612-45a5-a6ce-bd605ead994e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}\n"
     ]
    }
   ],
   "source": [
    "print(repository_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "539f32b3-85a3-437e-89f2-abc6f2e27505",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_data = []\n",
    "\n",
    "# Create a ZipFile object from the downloaded content\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    # Only process markdown files\n",
    "    if not (filename.endswith('.md') or filename.endswith('.mdx')):\n",
    "        continue\n",
    "\n",
    "    # Read and parse each file\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read()\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c67c77e-905a-482d-9e6c-10efed345300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}\n"
     ]
    }
   ],
   "source": [
    "print(repository_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c012ee7-c0dd-4f73-832b-c52feed5b820",
   "metadata": {},
   "source": [
    "#### Complete Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e84eda97-1720-4fe7-a15a-e2a66888316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26ed19bb-407b-414f-a034-f1085575599c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1219\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac165352-75b3-4147-bfc1-5c45769e740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "print(f\"Evidently documents: {len(evidently_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f318ae7-29d1-483a-83df-e84517b8fce0",
   "metadata": {},
   "source": [
    "### 1. Simple Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a8a7096-fb1c-45f2-9ff7-c047126e60a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a3078a8-c667-4497-bf62-08409b74e6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in dtc_faq:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1ffe62-c72a-4af4-89e3-8b32dabeea75",
   "metadata": {},
   "source": [
    "### 2. Splitting by Paragraphs and Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b384d-ff9d-49a0-8994-3dd7810c51b5",
   "metadata": {},
   "source": [
    "import re\n",
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94415635-752c-4e30-a201-f2434787c0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f624de8-a812-485b-9425-7ade5337c922",
   "metadata": {},
   "source": [
    "# Heading 1\n",
    "## Heading 2  \n",
    "### Heading 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c7d9f62-ff67-4b87-b45d-19fad96a15f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22ba973c-80c8-449f-8485-6c5595515057",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = split_markdown_by_level(text, level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ddf1fc7-4162-4a10-b0a9-f3debf70876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in dtc_faq:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a322b63-8868-4cf9-b984-c0ec6c409dd4",
   "metadata": {},
   "source": [
    "### 3. Intelligent Chunking with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "259ecc11-545b-4043-9b9e-9f9a470a9c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from groq import Groq\n",
    "\n",
    "#groq_client = Groq()\n",
    "\n",
    "\n",
    "#def llm(prompt, model='gpt-4o-mini'):\n",
    "    #messages = [\n",
    "     #   {\"role\": \"user\", \"content\": prompt}\n",
    "    #]\n",
    "\n",
    "    #response = groq_client.responses.create(\n",
    "     #   model='gpt-4o-mini',\n",
    "      #  input=messages\n",
    "    #)\n",
    "\n",
    "    #return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45542830-f118-4355-a238-6f1bcf381356",
   "metadata": {},
   "outputs": [
    {
     "ename": "GroqError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mGroqError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgroq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Groq\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m groq_client = \u001b[43mGroq\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# api_key=\"your-api-key Or use environment variable\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mllm\u001b[39m(prompt, model=\u001b[33m'\u001b[39m\u001b[33mllama-3.1-8b-instant\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m      6\u001b[39m     messages = [\n\u001b[32m      7\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}\n\u001b[32m      8\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/ai_agent_course_aihero/aihero/course/.venv/lib/python3.12/site-packages/groq/_client.py:79\u001b[39m, in \u001b[36mGroq.__init__\u001b[39m\u001b[34m(self, api_key, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m     77\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mGROQ_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GroqError(\n\u001b[32m     80\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     81\u001b[39m     )\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mGroqError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "groq_client = Groq()  # api_key=\"your-api-key Or use environment variable\n",
    "\n",
    "def llm(prompt, model='llama-3.1-8b-instant'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde70d4b-1525-4347-b1a7-1c40cd32143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8afe005-ec1f-43d5-abc2-64b25f33fd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e7c2c5-a6bf-4943-8987-713f650fb0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(dtc_faq):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0fd58f-e848-4d69-b9bf-b98493fdd007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19f256d7-02fe-474a-8990-0c7a2b44b305",
   "metadata": {},
   "source": [
    "Day 3: Add Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960e8c55-763a-48e5-82cb-13d4f7b83440",
   "metadata": {},
   "source": [
    "1. Text search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "503bd187-53e9-4d81-9327-a7f8e082b3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x72e82b3db9b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "index.fit(evidently_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa16c6b9-2aa4-4647-ac1e-0606c560ff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "results = index.search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0334f817-7b12-481d-a2eb-f5c312d3c555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'chunk': \"It's indeed good practice to only rely on the train dataset for EDA. Including validation might be okay. But we aren't supposed to touch the test dataset; even just looking at it isn't a good idea. We indeed pretend that this is the future unseen data.\",\n",
       "  'id': '0c6414cf51',\n",
       "  'question': 'What data should be used for EDA?',\n",
       "  'sort_order': 7,\n",
       "  'filename': 'faq-main/_questions/machine-learning-zoomcamp/module-3/007_0c6414cf51_what-data-should-be-used-for-eda.md'},\n",
       " {'start': 0,\n",
       "  'chunk': 'In Module 4, you’ll learn the core evaluation concepts used in classification tasks. Topics include:\\n- Metrics and diagnostics: precision, recall, ROC curves, and precision-recall curves\\n- Evaluation mindsets: how to think critically about metrics and validation in ML projects\\n- Common pitfalls: data leakage, improper validation, misinterpretation of metrics and curves\\n- Practical interpretation: selecting metrics based on context (class balance, costs of errors) and conveying results clearly\\n- Real-world applicability: how these concepts guide model comparison, threshold selection, and reporting\\nThis module is designed to be conceptual and abstract, yet practical for real-world ML work.',\n",
       "  'id': 'afd915ae5b',\n",
       "  'question': 'What exactly is taught in the evaluation module?',\n",
       "  'sort_order': 1,\n",
       "  'filename': 'faq-main/_questions/machine-learning-zoomcamp/module-4/001_afd915ae5b_module-4-evaluation-overview.md'},\n",
       " {'start': 0,\n",
       "  'chunk': \"No, you don't have to pay for this service in order to complete the course homeworks. You could use some of the free alternatives listed in the course GitHub.\\n\\n[llm-zoomcamp/01-intro/open-ai-alternatives.md at main · DataTalksClub/llm-zoomcamp (github.com)](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/open-ai-alternatives.md)\",\n",
       "  'id': '85384a18e5',\n",
       "  'question': 'OpenAI: Do I have to subscribe and pay for Open AI API for this course?',\n",
       "  'sort_order': 6,\n",
       "  'filename': 'faq-main/_questions/llm-zoomcamp/module-1/006_85384a18e5_openai-do-i-have-to-subscribe-and-pay-for-open-ai.md'},\n",
       " {'start': 0,\n",
       "  'chunk': 'In the macro `test_accepted_values` (found in `tests/generic/builtin.sql`), an error was triggered by the test `accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_` located in `models/staging/schema.yml`.\\n\\nTo resolve this issue, ensure the following variable is added to your `dbt_project.yml` file:\\n\\n```yaml\\nvars:\\n  payment_type_values: [1, 2, 3, 4, 5, 6]\\n```',\n",
       "  'id': '8bfd724e4f',\n",
       "  'question': \"Compilation Error in test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)  'NoneType' object is not iterable\",\n",
       "  'sort_order': 38,\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/038_8bfd724e4f_compilation-error-in-test-accepted_values_stg_gree.md'},\n",
       " {'start': 0,\n",
       "  'chunk': 'The image dataset for the competition was in a different layout from what we used in the dino vs dragon lesson. Since that’s what was covered, some folks were more comfortable with that setup, so a script was written to generate it for them.\\n\\nIt can be found here: [kitchenware-dataset-generator | Kaggle](https://www.kaggle.com/code/clamytoe/kitchenware-dataset-generator)',\n",
       "  'id': '0156cf3b62',\n",
       "  'question': 'Kitchenware Classification Competition Dataset Generator',\n",
       "  'sort_order': 18,\n",
       "  'filename': 'faq-main/_questions/machine-learning-zoomcamp/misc/018_0156cf3b62_kitchenware-classification-competition-dataset-gen.md'},\n",
       " {'start': 0,\n",
       "  'chunk': \"You can get a few cloud points by using Kubernetes even if you deploy it only locally. Alternatively, you can use LocalStack to mimic AWS. Be sure you're clear on the [Evaluation Criteria](https://github.com/DataTalksClub/mlops-zoomcamp/tree/main/07-project#evaluation-criteria).\",\n",
       "  'id': 'f6544a339e',\n",
       "  'question': 'Project: For the final project, is it required to be put on the cloud?',\n",
       "  'sort_order': 21,\n",
       "  'filename': 'faq-main/_questions/mlops-zoomcamp/general/021_f6544a339e_project-for-the-final-project-is-it-required-to-be.md'},\n",
       " {'start': 0,\n",
       "  'chunk': 'You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards, as you did in Question 2.',\n",
       "  'id': '7c90af9692',\n",
       "  'question': 'What dataset should I use to compute the metrics in Question 3',\n",
       "  'sort_order': 3,\n",
       "  'filename': 'faq-main/_questions/machine-learning-zoomcamp/module-4-homework/003_7c90af9692_what-dataset-should-i-use-to-compute-the-metrics-i.md'},\n",
       " {'start': 0,\n",
       "  'chunk': \"### Problem\\n\\nRMSE on the test set was too high when hot encoding the validation set using a previously fitted `OneHotEncoder(handle_unknown='ignore')` on the training set. In contrast, `DictVectorizer` yielded the correct RMSE.\\n\\n### Explanation\\n\\nIn principle, both transformers should behave identically when treating categorical features, especially in scenarios where there are no sequences of strings in each row (as in this week’s homework):\\n\\n- Features are put into binary columns encoding their presence (1) or absence (0).\\n- Unknown categories are imputed as zeros in the hot-encoded matrix.\\n\\nThis discrepancy indicates that there might be a difference in how `OneHotEncoder` and `DictVectorizer` handle the data after fitting on the training set and applying to the validation set.\",\n",
       "  'id': '886fddb311',\n",
       "  'question': 'RMSE on test set too high',\n",
       "  'sort_order': 30,\n",
       "  'filename': 'faq-main/_questions/mlops-zoomcamp/module-1/030_886fddb311_rmse-on-test-set-too-high.md'},\n",
       " {'start': 0,\n",
       "  'chunk': \"Q2 asks about the correlation matrix and converting `median_house_value` from numeric to binary. Just to clarify, we are only dealing with `df_train`, not `df_train_full`, correct? The question explicitly mentions the train dataset.\\n\\nYes, it is only on `df_train`. The reason is that `df_train_full` also contains the validation dataset. At this stage, we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data.\",\n",
       "  'id': '71f360d8eb',\n",
       "  'question': 'What data should we use for correlation matrix?',\n",
       "  'sort_order': 1,\n",
       "  'filename': 'faq-main/_questions/machine-learning-zoomcamp/module-3-homework/001_71f360d8eb_what-data-should-we-use-for-correlation-matrix.md'},\n",
       " {'start': 4000,\n",
       "  'chunk': 'les and directories\\n- Binary files in question directories\\n- Malformed markdown content\\n\\n### Real-World Scenarios\\nIntegration tests simulate:\\n- Large courses with 10+ sections and 50+ questions\\n- International content with unicode characters\\n- Complex markdown with tables, task lists, and code blocks\\n- Mixed valid/invalid content in the same course\\n\\n## Adding New Tests\\n\\nWhen adding new functionality to `generate_website.py`:\\n\\n1. **Add unit tests** for new functions in appropriate `test_*.py` files\\n2. **Add integration tests** if the change affects the complete workflow\\n3. **Test edge cases** and error conditions\\n4. **Update this README** if new test categories are added\\n\\n### Test Naming Convention\\n- Test files: `test_<functionality>.py`\\n- Test classes: `Test<ClassName>`\\n- Test methods: `test_<specific_behavior>`\\n\\n### Example Test Structure\\n```python\\nclass TestNewFeature:\\n    \"\"\"Test the new feature functionality\"\"\"\\n    \\n    def test_basic_functionality(self):\\n        \"\"\"Test basic feature behavior\"\"\"\\n        pass\\n        \\n    def test_edge_cases(self):\\n        \"\"\"Test edge cases and error conditions\"\"\"\\n        pass\\n        \\n    def test_integration_with_existing_features(self):\\n        \"\"\"Test how feature integrates with existing code\"\"\"\\n        pass\\n```\\n\\n## Dependencies\\n\\nThe test suite requires:\\n- `pytest>=7.0.0` (defined in pyproject.toml dev dependencies)\\n- All main project dependencies for testing the actual functionality\\n\\n## Configuration\\n\\nTest configuration is defined in `pyproject.toml`:\\n- Test discovery paths\\n- Test file patterns\\n- Output formatting\\n- Custom markers for test categorization',\n",
       "  'filename': 'faq-main/tests/README.md'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9a7447a-a43a-454e-8dbd-9e971c33a808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x72e82b3bbd70>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "\n",
    "faq_index = Index(\n",
    "    text_fields=[\"question\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "faq_index.fit(de_dtc_faq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21024ef3-1527-4a4d-9e3e-e9730ae8f088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x72e82c7fd1f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faq_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f9da7d9-cc15-41c8-a9c1-f47a0d42e533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '8bfd724e4f',\n",
       "  'question': \"Compilation Error in test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)  'NoneType' object is not iterable\",\n",
       "  'sort_order': 38,\n",
       "  'content': 'In the macro `test_accepted_values` (found in `tests/generic/builtin.sql`), an error was triggered by the test `accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_` located in `models/staging/schema.yml`.\\n\\nTo resolve this issue, ensure the following variable is added to your `dbt_project.yml` file:\\n\\n```yaml\\nvars:\\n  payment_type_values: [1, 2, 3, 4, 5, 6]\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/038_8bfd724e4f_compilation-error-in-test-accepted_values_stg_gree.md'},\n",
       " {'id': 'c180431de3',\n",
       "  'question': 'DBT Cloud production error: prod dataset not available in location EU',\n",
       "  'sort_order': 4,\n",
       "  'content': \"**Problem:**\\n\\nI am trying to deploy my DBT models to production using DBT Cloud. The data should reside in BigQuery with the dataset location as EU. However, when running the model in production, a prod dataset is incorrectly created in BigQuery with a location of US. This leads to the build failing with the error:\\n\\n```\\nERROR 404: project.dataset:prod not available in location EU\\n```\\n\\nI have attempted various fixes, but I'm unsure if there is a simpler solution than creating my projects or buckets in the US location.\\n\\nNote: Everything functions properly in development mode; the issue arises only during job scheduling and execution in production.\\n\\n**Solution:**\\n\\n1. Manually create the `prod` dataset in BigQuery with the EU location specified.\\n2. Rerun the production job.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/004_c180431de3_dbt-cloud-production-error-prod-dataset-not-availa.md'},\n",
       " {'id': '9f9a1b9e4f',\n",
       "  'question': 'Terraform: Teardown of BigQuery Dataset',\n",
       "  'sort_order': 126,\n",
       "  'content': \"When running `terraform destroy`, the following error can occur:\\n\\n```\\nDo you really want to destroy all resources?\\n\\nTerraform will destroy all your managed infrastructure, as shown above.\\n\\nThere is no undo. Only 'yes' will be accepted to confirm.\\n\\nEnter a value: yes\\n\\ngoogle_bigquery_dataset.homework_dataset: Destroying... [id=projects/terraform-demo-449214/datasets/homework_dataset]\\n\\n╷\\n\\n│ Error: Error when reading or editing Dataset: googleapi: Error 400: Dataset terraform-demo-449214:homework_dataset is still in use, resourceInUse\\n```\\n\\nThis is because the dataset is still in use by a table. To delete the dataset, set the `delete_contents_on_destroy` property to `true` in the `main.tf` file.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/126_9f9a1b9e4f_terraform-teardown-of-bigquery-dataset.md'},\n",
       " {'id': '04f55d5846',\n",
       "  'question': 'BigQuery adapter: 404 Not found: Dataset was not found in location europe-west6',\n",
       "  'sort_order': 32,\n",
       "  'content': '1. Go to **Account settings** > **Project** > **Analytics**.\\n2. Click on your connection.\\n3. Scroll down to **Location** and type in the GCP location exactly as displayed in GCP (e.g., `europe-west6`). You might need to reupload your GCP key.\\n\\n4. Delete your dataset in Google BigQuery (GBQ).\\n5. Rebuild the project using the command:\\n   \\n   ```bash\\n   dbt build\\n   ```\\n\\n6. The newly built dataset should be in the correct location.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/032_04f55d5846_bigquery-adapter-404-not-found-dataset-was-not-fou.md'},\n",
       " {'id': 'e7738f47c8',\n",
       "  'question': 'Should I pay for cloud services?',\n",
       "  'sort_order': 28,\n",
       "  'content': \"It's not mandatory. You can take advantage of their free trial.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/028_e7738f47c8_should-i-pay-for-cloud-services.md'},\n",
       " {'id': '05727a95dd',\n",
       "  'question': 'Homework and Leaderboard: What is the system for points in the course management',\n",
       "  'sort_order': 18,\n",
       "  'content': \"After you submit your homework, it will be graded based on the number of questions in that particular assignment. You can see the number of points you have earned at the top of the homework page. Additionally, in the [leaderboard](https://courses.datatalks.club/de-zoomcamp-2025/leaderboard), you will find the sum of all points you've earned: points for Homeworks, FAQs, and Learning in Public.\\n\\nPoint System Overview:\\n\\n- **Homework:** Points vary by assignment based on the number of questions.\\n- **FAQ Contribution:** You get a maximum of 1 point for contributing to the FAQ in the respective week.\\n- **Learning in Public:** For each learning in public link, you earn one point. You can achieve a maximum of 7 points.\\n\\nCheck this [video](https://www.loom.com/share/710e3297487b409d94df0e8da1c984ce) for more details.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/05727a95dd_homework-and-leaderboard-wha.md'},\n",
       " {'id': 'c18119ba32',\n",
       "  'question': 'Invalid dataset ID Error when running the gcp_setup flow',\n",
       "  'sort_order': 10,\n",
       "  'content': 'When following the [YouTube lesson](https://www.youtube.com/watch?v=nKqjjLJ7YXs&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=23) and then running the [gcp_setup flow](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/02-workflow-orchestration/flows/05_gcp_setup.yaml), the error occurs during the `create_bq_dataset` task.\\n\\nThe error is less clear, but it stems from using a dash in the dataset name. To resolve this, change the dataset name to something like \"de_zoomcamp\" to avoid using a dash. This should resolve the error.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-2/010_c18119ba32_invalid-dataset-id-error-when-running-the-gcp_setu.md'},\n",
       " {'id': '0f6e2d3813',\n",
       "  'question': 'How do I solve the Dbt Cloud error: prod was not found in location?',\n",
       "  'sort_order': 5,\n",
       "  'content': \"You might encounter this error when trying to run dbt in production after following the instructions in the video ‘DE Zoomcamp 4.4.1 - Deployment Using dbt Cloud (Alternative A’):\\n\\n```\\nDatabase Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)\\nNot found: Dataset module-4-analytics-eng:prod was not found in location europe-west10\\n```\\n\\nThis error is easily resolved. Here are two solutions to address this issue:\\n\\n**Solution #1: Matching the dataset's data location with the source dataset**\\n\\n- Set your ‘prod’ dataset's data location to match the data location of your ‘trips_data_all’ dataset in BigQuery. The dbt process works for the instructor because her ‘prod’ dataset is in the same region as her source data. Since your ‘trips_data_all’ is in europe-west10 (or another region besides US), your prod needs to be there too, not US (which is the default setting when dbt creates a dataset for you in BigQuery).\\n\\n**Solution #2: Changing the dataset to <development dataset>**\\n\\n1. Go to: Deploy / Environments / Production (your production environment) / Settings.\\n2. Look at the Deployment credentials. There is an input field called Dataset. The input of ‘prod’ is likely here.\\n3. Replace ‘prod’ with the name of the Dataset that you worked with during development (before moving to Production). This is the Dataset name inside your BigQuery where you successfully ran ‘dbt debug’ and ‘dbt build’.\\n4. After saving, you are ready to rerun your Job!\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/005_0f6e2d3813_how-do-i-solve-the-dbt-cloud-error-prod-was-not-fo.md'},\n",
       " {'id': 'd7abe28e77',\n",
       "  'question': 'Which set-up should I use for my dlt homework?',\n",
       "  'sort_order': 2,\n",
       "  'content': 'Technically you can use any code editor or Jupyter Notebook, as long as you can run dbt and answer the homework questions. A lot of code is provided by the instructor on the homework page to give you a head start in the right direction: [dlt Homework Instructions](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2025/workshops/dlt/dlt_homework.md).\\n\\nThe most practical way is to use the provided Colabs Jupyter notebook called ‘dlt - Homework.ipynb’ which you can find here: [Colab Notebook](https://colab.research.google.com/drive/1plqdl33K_HkVx0E0nGJrrkEUssStQsW7#scrollTo=BtsSxtFfXQs3) since all of the provided code is applicable in the Colabs set-up.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/002_d7abe28e77_which-set-up-should-i-use-for-my-dlt-homework.md'},\n",
       " {'id': 'bfafa427b3',\n",
       "  'question': 'Course: What are the prerequisites for this course?',\n",
       "  'sort_order': 2,\n",
       "  'content': 'To get the most out of this course, you should have:\\n\\n- Basic coding experience\\n- Familiarity with SQL\\n- Experience with Python (helpful but not required)\\n\\nNo prior data engineering experience is necessary. See [Readme on GitHub](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/README.md#prerequisites).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/002_bfafa427b3_course-what-are-the-prerequisites-for-this-course.md'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "results2 = faq_index.search(query)\n",
    "results2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b137f926-00b8-46f3-aebd-4b921f500336",
   "metadata": {},
   "source": [
    "2. Vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "accca461-94fd-485c-917d-aad37b6c7a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m149 packages\u001b[0m \u001b[2min 820ms\u001b[0m\u001b[0m                                       \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m24 packages\u001b[0m \u001b[2min 3m 23s\u001b[0m\u001b[0m                                           \n",
      "\u001b[2K░░░░░░░░░░░░░░░░░░░░ [0/29] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m29 packages\u001b[0m \u001b[2min 5m 04s\u001b[0m\u001b[0m                             \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.19.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.1.10\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.35.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.8.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.3.83\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.13.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.9.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.3.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==11.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2025.9.18\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.6.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msentence-transformers\u001b[0m\u001b[2m==5.1.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.4.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9b96e2d-957a-4164-876d-93ff8853ce92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b71c27ba5146cca8cd136934246a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdab0d835d6e48338d5dfbbd6bbdea65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8500e4b930eb4fed924689fa00c00b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89326a592ba48cdb34bb8d1216cbce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d913f62b58f442fa277807e0060795e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/523 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88577c4c88348e19335b55ff933e2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fb9a5ab3784a328dd4b2e9538a335b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749336b291bd489f9fdb4a4f1c3a8af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154cc1dadf4d4c8a98d968503ed11a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4855ad257dd463dac9777b75c9ca93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d07153503f34036b226b5208c4431ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f823c93b-49a8-4fb2-b5ae-171ff96d0f1c",
   "metadata": {},
   "source": [
    "The multi-qa-distilbert-cos-v1 model is trained explicitly for question-answering tasks. It creates embeddings optimized for finding answers to questions.\n",
    "Other popular models include:\n",
    "- all-MiniLM-L6-v2 - General-purpose, fast, and efficient\n",
    "- all-mpnet-base-v2 - Higher quality, slower\n",
    "\n",
    "Check Sentence Transformers documentation for more options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e500e677-d0de-494d-9f1f-47bbe4483dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = de_dtc_faq[2]\n",
    "text = record['question'] + ' ' + record['content']\n",
    "v_doc = embedding_model.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a5d76284-dd6c-485f-8c6d-ede823a401e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'I just found out about the course. Can I enroll now?'\n",
    "v_query = embedding_model.encode(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2851cf71-7d24-47ce-af18-d723617258f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6477411f136440b3981515ad76881cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "faq_embeddings = []\n",
    "\n",
    "for d in tqdm(de_dtc_faq):\n",
    "    text = d['question'] + ' ' + d['content']\n",
    "    v = embedding_model.encode(text)\n",
    "    faq_embeddings.append(v)\n",
    "\n",
    "faq_embeddings = np.array(faq_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7658c2b-d0e8-41c5-bda6-f73be73e40a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x72e8277aa120>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import VectorSearch\n",
    "\n",
    "faq_vindex = VectorSearch()\n",
    "faq_vindex.fit(faq_embeddings, de_dtc_faq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebe9d9b7-34d2-4905-82ec-70441e547d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Can I join the course now?'\n",
    "q = embedding_model.encode(query)\n",
    "results = faq_vindex.search(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86533c72-a526-4c51-ad12-025ed64a6346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '3f1424af17',\n",
       "  'question': 'Course: Can I still join the course after the start date?',\n",
       "  'sort_order': 3,\n",
       "  'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'},\n",
       " {'id': '068529125b',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'sort_order': 8,\n",
       "  'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'},\n",
       " {'id': '9e508f2212',\n",
       "  'question': 'Course: When does the course start?',\n",
       "  'sort_order': 1,\n",
       "  'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'},\n",
       " {'id': 'c207b8614e',\n",
       "  'question': 'Course: Can I get support if I take the course in the self-paced mode?',\n",
       "  'sort_order': 9,\n",
       "  'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'},\n",
       " {'id': '900f60fd25',\n",
       "  'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?',\n",
       "  'sort_order': 15,\n",
       "  'content': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md'},\n",
       " {'id': '721f9e0c29',\n",
       "  'question': 'How can we contribute to the course?',\n",
       "  'sort_order': 35,\n",
       "  'content': '- [Star the repository](https://github.com/DataTalksClub/data-engineering-zoomcamp).\\n- Share it with friends if you find it useful.\\n- Create a pull request (PR) if you can improve the text or structure of the repository.\\n- [Update this FAQ](https://github.com/DataTalksClub/faq/).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/035_721f9e0c29_how-can-we-contribute-to-the-course.md'},\n",
       " {'id': '6314bc3029',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_1813f02b.png'}],\n",
       "  'question': 'How do I get my certificate?',\n",
       "  'sort_order': 46,\n",
       "  'content': 'There\\'ll be an announcement in Telegram and the course channel for:\\n\\n- Checking that your full name is displayed correctly on the Certificate (see Editing course profile on the Course Management webpage).\\n- Notifying when the grading is completed.\\n\\nYou will find it in your course profile (you need to be\\nlogged it). \\n\\nFor 2025 the link to the course profile is this:\\n\\n`https://courses.datatalks.club/de-zoomcamp-2025/enrollment`\\n\\nFor other editions, change \"2025\" to your edition.\\n\\nAfter the second announcement, follow instructions in [certificates.md](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/certificates.md) on how to generate the Certificate document yourself.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/046_6314bc3029_how-do-i-get-my-certificate.md'},\n",
       " {'id': '16005581f2',\n",
       "  'question': 'Edit Course Profile.',\n",
       "  'sort_order': 13,\n",
       "  'content': 'The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname or your real name if you prefer. Your entry on the Leaderboard is the one highlighted in light green.\\n\\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\\n\\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/013_16005581f2_edit-course-profile.md'},\n",
       " {'id': 'b7542b8d36',\n",
       "  'question': 'Environment: Is the course [Windows/macOS/Linux/...] friendly?',\n",
       "  'sort_order': 36,\n",
       "  'content': 'Yes! Linux is ideal but technically it should not matter. Students in the 2024 cohort used all 3 OSes successfully.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/036_b7542b8d36_environment-is-the-course-windowsmacoslinux-friend.md'},\n",
       " {'id': 'dc06a38bc6',\n",
       "  'question': 'How do I use Git / GitHub for this course?',\n",
       "  'sort_order': 42,\n",
       "  'content': 'After you create a GitHub account, clone the course repo to your local machine using the process outlined in this video:\\n\\n[Git for Everybody: How to Clone a Repository from GitHub](https://www.youtube.com/watch?v=CKcqniGu3tA).\\n\\nHaving this local repository on your computer will make it easy to access the instructors’ code and make pull requests if you want to add your own notes or make changes to the course content.\\n\\nYou will probably also create your own repositories to host your notes and versions of files. Here is a great tutorial that shows you how to do this:\\n\\n[How to Create a Git Repository](https://www.atlassian.com/git/tutorials/setting-up-a-repository).\\n\\nRemember to ignore large databases, .csv, and .gz files, and other files that should not be saved to a repository. Use `.gitignore` for this:\\n\\n[.gitignore file](https://www.atlassian.com/git/tutorials/saving-changes/gitignore).\\n\\n**Important:**\\n\\n**NEVER store passwords or keys in a git repo** (even if the repo is set to private). Put files containing sensitive information (.env, secret.json, etc.) in your `.gitignore`.\\n\\nThis is also a great resource: [Dangit, Git!?!](https://dangitgit.com/)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/042_dc06a38bc6_how-do-i-use-git-github-for-this-course.md'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf38f08-cb13-42aa-b3af-89bf061d98fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
