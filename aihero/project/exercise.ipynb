{
 "cells": [
  {
   "cell_type": "raw",
   "id": "482f3f2f-0ac0-4048-a561-1b4e3cf2826e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Getting Started with AI\"\n",
    "author: \"John Doe\"\n",
    "date: \"2024-01-15\"\n",
    "tags: [\"ai\", \"machine-learning\", \"tutorial\"]\n",
    "difficulty: \"beginner\"\n",
    "---\n",
    "\n",
    "# Getting Started with AI\n",
    "\n",
    "This is the main content of the document written in **Markdown**.\n",
    "\n",
    "You can include code blocks, links, and other formatting here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2bc609f-4858-43f6-9f2e-72b7d41d534c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mResolved \u001b[1m110 packages\u001b[0m \u001b[2min 0.88ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m19 packages\u001b[0m \u001b[2min 0.50ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2780fa80-db34-4bd2-9119-d21215eec975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87eebc54-eed3-4a17-8876-999e512112f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 4\n",
      "Evidently documents: 93\n"
     ]
    }
   ],
   "source": [
    "brdt_mcp = read_repo_data('brightdata', 'brightdata-mcp')\n",
    "ai_docs = read_repo_data('patchy631', 'ai-engineering-hub')\n",
    "\n",
    "print(f\"FAQ documents: {len(brdt_mcp)}\")\n",
    "print(f\"Evidently documents: {len(ai_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16c0ca41-ee4b-43e3-8028-ffc1336cbc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "groq_client = Groq()  # Or use os.environ.get(\"GROQ_API_KEY\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()\n",
    "\n",
    "def llm(document, model='llama-3.1-8b-instant'):\n",
    "    prompt = prompt_template.format(document=document)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that organizes documents for Q&A systems.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20184a68-1cc8-4975-987f-68c88aaeb6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "groq_client = Groq()  # api_key=\"your-api-key Or use environment variable\n",
    "\n",
    "def llm(prompt, model='llama-3.1-8b-instant'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0551eadc-74cf-4e48-80e3-c223489ee3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88d3c783-b01b-4b91-9a6f-80b51614c905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92875b22-5a4f-48ef-a388-b86d3aa6885b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m111 packages\u001b[0m \u001b[2min 53ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/1] \u001b[2mInstalling wheels...                                 \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 67ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv add tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76df350b-d806-4115-bf97-96071c616242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54046e133b094faf9f01e3c116c0a09d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(brdt_mcp):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63fd39a9-eddf-40d6-a25e-347995a5465c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'filename': 'brightdata-mcp-main/CHANGELOG.md',\n",
       "  'section': '## Project Changelogs\\n\\nThe changelog document contains records of all the notable changes made to the project, including updates, bug fixes, and new features.'},\n",
       " {'filename': 'brightdata-mcp-main/CHANGELOG.md',\n",
       "  'section': '## Release Notes for Version 1.9.x Series\\n\\nThe 1.9.x series focuses on expanding web data collection capabilities and improving authentication mechanisms. Key highlights include the addition of 23 new web data tools.\\n\\n### Changed\\n\\n- Updated browser authentication to use API_TOKEN instead of previous authentication method\\n- BROWSER_ZONE is now an optional parameter, the default zone is `mcp_browser`\\n- Removed duplicate web_data_ tools\\n- Updated coding conventions and file formatting\\n- Enhanced web data API endpoints integration\\n\\n### Fixed\\n\\n- Fixed spelling errors and improved coding conventions\\n- Converted files back to Unix line endings for consistency'},\n",
       " {'filename': 'brightdata-mcp-main/CHANGELOG.md',\n",
       "  'section': '## Release Notes for Version 1.8.x Series\\n\\nThe 1.8.x series introduced significant improvements to browser session management, WSAPI endpoints, and overall system reliability. Notable features include domain-based sessions and automatic zone creation.\\n\\n### Changed\\n\\n- Bumped FastMCP version for improved performance\\n- Updated README.md with additional documentation\\n- Applied dos2unix formatting for consistency\\n- Updated Docker configuration\\n- Updated smithery.yaml configuration\\n\\n### Added\\n\\n- Added Bright Data MCP with Claude demo video to README.md\\n- Added automatic creation of required unlocker zone when not present\\n- Added 12 new WSAPI endpoints for enhanced functionality\\n- Changed to polling mechanism for better reliability\\n- Added domain-based browser sessions to avoid navigation limit issues\\n\\n### Fixed\\n\\n- Fixed GitHub references and repository settings\\n- Fixed browser context maintenance across tool calls with current domain tracking\\n- Minor lint fixes'},\n",
       " {'filename': 'brightdata-mcp-main/CHANGELOG.md',\n",
       "  'section': '## Release Notes for Version 1.0.0\\n\\nInitial stable release providing core MCP server functionality for Bright Data integration with comprehensive browser automation and web scraping capabilities.\\n\\n### Added\\n\\n- Initial release of Bright Data MCP server\\n- Browser automation capabilities with Bright Data integration\\n- Core web scraping and data collection tools\\n- Smithery.yaml configuration for deployment in Smithery.ai\\n- MIT License\\n- Demo materials and documentation\\n- Created comprehensive README.md\\n- Added demo.md with usage examples\\n- Created examples/README.md for sample implementations\\n- Added Tools.md documentation for available tools'},\n",
       " {'filename': 'brightdata-mcp-main/README.md',\n",
       "  'section': 'Here are the sections for the provided document:\\n\\n## Introduction\\n\\nThe Web MCP is a tool that provides your AI with real-time web capabilities. It allows you to give your AI true web capabilities, with no more outdated responses or \"I can\\'t access real-time information\" messages.\\n\\n### Key Features\\n\\n* Works with any LLM (Claude, GPT, Gemini, Llama)\\n* Ensures your AI never gets blocked, rate-limited, or served CAPTCHAs\\n* Provides 5,000 free requests per month\\n\\n### Benefits\\n\\n* Perfect for real-time research, e-commerce intelligence, market analysis, AI agents, content creation, and academic research'},\n",
       " {'filename': 'brightdata-mcp-main/README.md',\n",
       "  'section': '## Quick Start\\n\\n### Use Our Hosted Server\\n\\nUse our hosted server with no installation needed! Just add this URL to your MCP client:\\n\\n```\\nhttps://mcp.brightdata.com/mcp?token=YOUR_API_TOKEN_HERE\\n```\\n\\n### Run Locally on Your Machine\\n\\n```json\\n{\\n  \"mcpServers\": {\\n    \"Bright Data\": {\\n      \"command\": \"npx\",\\n      \"args\": [\"@brightdata/mcp\"],\\n      \"env\": {\\n        \"API_TOKEN\": \"<your-api-token-here>\"\\n      }\\n    }\\n  }\\n}\\n```'},\n",
       " {'filename': 'brightdata-mcp-main/README.md',\n",
       "  'section': '## Pricing & Modes\\n\\n### Pricing\\n\\n* Rapid Mode (Free): $0/month for 5,000 requests\\n* Pro Mode: Pay-as-you-go for every tool and feature\\n\\n### Modes\\n\\n| Mode | Description | Tools Included |\\n|'},\n",
       " {'filename': 'brightdata-mcp-main/README.md', 'section': '|'},\n",
       " {'filename': 'brightdata-mcp-main/README.md', 'section': '-|'},\n",
       " {'filename': 'brightdata-mcp-main/README.md',\n",
       "  'section': '-|\\n| Rapid Mode | Default mode for free tier users | üîç `search_engine`, üìÑ `scrape_as_markdown` |\\n| Pro Mode | Advanced tools for paid users | Browser control, web data APIs, e-commerce tools, and more |'},\n",
       " {'filename': 'brightdata-mcp-main/README.md',\n",
       "  'section': '## Features\\n\\n### Core Capabilities\\n\\n| Feature | Description |\\n|'},\n",
       " {'filename': 'brightdata-mcp-main/README.md', 'section': '|'},\n",
       " {'filename': 'brightdata-mcp-main/README.md',\n",
       "  'section': '-|\\n| üîç Smart Web Search | Google-quality results optimized for AI |\\n| üìÑ Clean Markdown | AI-ready content extraction |\\n| üåç Global Access | Bypass geo-restrictions automatically |\\n| üõ°Ô∏è Anti-Bot Protection | Never get blocked or rate-limited |\\n\\n### Browser Automation\\n\\n| Feature | Description |\\n|'},\n",
       " {'filename': 'brightdata-mcp-main/README.md', 'section': '|'},\n",
       " {'filename': 'brightdata-mcp-main/README.md',\n",
       "  'section': '-|\\n| ü§ñ Browser Automation | Control real browsers remotely (Pro mode only) |\\n\\n### Lightning Fast\\n\\n| Feature | Description |\\n|'},\n",
       " {'filename': 'brightdata-mcp-main/README.md', 'section': '|'},\n",
       " {'filename': 'brightdata-mcp-main/README.md',\n",
       "  'section': '-|\\n| ‚ö° Lightning Fast | Optimized for minimal latency |'},\n",
       " {'filename': 'brightdata-mcp-main/README.md',\n",
       "  'section': '## Demos\\n\\n* Watch demos on earlier versions: https://github.com/user-attachments/assets/59f6ebba-801a-49ab-8278-1b2120912e33\\n* View more tutorials on YouTube: https://github.com/brightdata-com/brightdata-mcp/blob/main/examples/README.md'},\n",
       " {'filename': 'brightdata-mcp-main/README.md',\n",
       "  'section': '## Available Tools\\n\\n### Rapid Mode Tools (Default - Free)\\n\\n| Tool | Description | Use Case |\\n|'},\n",
       " {'filename': 'brightdata-mcp-main/README.md', 'section': '|'},\n",
       " {'filename': 'brightdata-mcp-main/README.md', 'section': '-|'},\n",
       " {'filename': 'brightdata-mcp-main/README.md',\n",
       "  'section': '-|\\n| üîç `search_engine` | Web search with AI-optimized results | Research, fact-checking, current events |\\n| üìÑ `scrape_as_markdown` | Convert any webpage to clean markdown | Content extraction, documentation |\\n\\n### Pro Mode Tools (60+ Tools)\\n\\n| Category | Tools | Description |\\n|'},\n",
       " {'filename': 'brightdata-mcp-main/README.md', 'section': '-|'},\n",
       " {'filename': 'brightdata-mcp-main/README.md', 'section': '-|'},\n",
       " {'filename': 'brightdata-mcp-main/README.md',\n",
       "  'section': '-|\\n| Browser Control | `scraping_browser.*` | Full browser automation |\\n| Web Data APIs | `web_data_*` | Structured data extraction |\\n| E-commerce | Product scrapers | Amazon, eBay, Walmart data |\\n| Social Media | Social scrapers | Twitter, LinkedIn, Instagram |\\n| Maps & Local | Location tools | Google Maps, business data |'},\n",
       " {'filename': 'brightdata-mcp-main/README.md',\n",
       "  'section': '## Try It Now!\\n\\n### Online Playground\\n\\nTry the Web MCP without any setup:\\n\\n<div align=\"center\">\\n  <a href=\"https://brightdata.com/ai/playground-chat\">\\n    <img src=\"https://img.shields.io/badge/Try_on-Playground-00C7B7?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMjQiIGhlaWdodD0iMjQiIHZpZXdCb3g9IjAgMCAyNCAyNCIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPHBhdGggZD0iTTEyIDJMMyA3VjE3TDEyIDIyTDIxIDE3VjdMMTIgMloiIHN0cm9rZT0id2hpdGUiIHN0cm9rZS13aWR0aD0iMiIvPgo8L3N2Zz4=\" alt=\"Playground\"/>\\n  </a>\\n</div>'},\n",
       " {'filename': 'brightdata-mcp-main/README.md',\n",
       "  'section': '## Configuration\\n\\n### Basic Setup\\n\\n```json\\n{\\n  \"mcpServers\": {\\n    \"Bright Data\": {\\n      \"command\": \"npx\",\\n      \"args\": [\"@brightdata/mcp\"],\\n      \"env\": {\\n        \"API_TOKEN\": \"your-token-here\"\\n      }\\n    }\\n  }\\n}\\n```\\n\\n### Advanced Configuration\\n\\n```json\\n{\\n  \"mcpServers\": {\\n    \"Bright Data\": {\\n      \"command\": \"npx\",\\n      \"args\": [\"@brightdata/mcp\"],\\n      \"env\": {\\n        \"API_TOKEN\": \"your-token-here\",\\n        \"PRO_MODE\": \"true\",              // Enable all 60+ tools\\n        \"RATE_LIMIT\": \"100/1h\",          // Custom rate limiting\\n        \"WEB_UNLOCKER_ZONE\": \"custom\",   // Custom unlocker zone\\n        \"BROWSER_ZONE\": \"custom_browser\" // Custom browser zone\\n      }\\n    }\\n  }\\n}\\n```'},\n",
       " {'filename': 'brightdata-mcp-main/README.md',\n",
       "  'section': '## Documentation\\n\\n* View the API documentation: https://docs.brightdata.com/mcp-server/overview\\n* View the example queries: https://github.com/brightdata-com/brightdata-mcp/blob/main/examples/README.md'},\n",
       " {'filename': 'brightdata-mcp-main/README.md',\n",
       "  'section': '## Common Issues & Solutions\\n\\n* View the troubleshooting guide: https://github.com/brightdata-com/brightdata-mcp#troubleshooting'},\n",
       " {'filename': 'brightdata-mcp-main/README.md',\n",
       "  'section': '## Contributing\\n\\n* Report bugs: https://github.com/brightdata-com/brightdata-mcp/issues\\n* Suggest features: https://github.com/brightdata-com/brightdata-mcp/issues\\n* Submit PRs: https://github.com/brightdata-com/brightdata-mcp/pulls'},\n",
       " {'filename': 'brightdata-mcp-main/README.md',\n",
       "  'section': '## Support\\n\\n* View the GitHub issues: https://github.com/brightdata-com/brightdata-mcp/issues\\n* View the documentation: https://docs.brightdata.com/mcp-server/overview\\n* Email support: support@brightdata.com'},\n",
       " {'filename': 'brightdata-mcp-main/README.md',\n",
       "  'section': '## License\\n\\nMIT Copyright (c) 2023 Bright Data Ltd.'},\n",
       " {'filename': 'brightdata-mcp-main/assets/Tools.md',\n",
       "  'section': '## Web Scraping and Data Extraction\\n\\nThe available features for web scraping and data extraction are:\\n\\n- **Search Engine**: Scrape search results from Google, Bing, or Yandex.\\n- **Scrape as Markdown**: Scrape a single webpage URL with advanced options for content extraction and get back the results in MarkDown language.\\n- **Scrape as HTML**: Scrape a single webpage URL with advanced options for content extraction and get back the results in HTML.\\n- **Scraping Browser Navigate**: Navigate a scraping browser session to a new URL.\\n- **Scraping Browser Go Back**: Go back to the previous page.\\n- **Scraping Browser Go Forward**: Go forward to the next page.\\n- **Scraping Browser Click**: Click on an element.\\n- **Scraping Browser Links**: Get all links on the current page, text, and selectors.\\n- **Scraping Browser Type**: Type text into an element.\\n- **Scraping Browser Wait for**: Wait for an element to be visible on the page.\\n- **Scraping Browser Screenshot**: Take a screenshot of the current page.\\n- **Scraping Browser Get HTML**: Get the HTML content of the current page.\\n- **Scraping Browser Get Text**: Get the text content of the current page.'},\n",
       " {'filename': 'brightdata-mcp-main/assets/Tools.md',\n",
       "  'section': '## Data Extraction from Amazon\\n\\nThe following features are available for extracting data from Amazon:\\n\\n- **Web Data Amazon Product**: Quickly read structured Amazon product data.\\n- **Web Data Amazon Product Reviews**: Quickly read structured Amazon product review data.\\n- **Web Data Amazon Product Search**: Quickly read structured Amazon product search data.'},\n",
       " {'filename': 'brightdata-mcp-main/assets/Tools.md',\n",
       "  'section': '## Data Extraction from Other E-commerce Platforms\\n\\nThe following features are available for extracting data from other e-commerce platforms:\\n\\n- **Web Data Walmart Product**: Quickly read structured Walmart product data.\\n- **Web Data Walmart Seller**: Quickly read structured Walmart seller data.\\n- **Web Data Ebay Product**: Quickly read structured eBay product data.\\n- **Web Data Homedepot Products**: Quickly read structured Home Depot product data.\\n- **Web Data Zara Products**: Quickly read structured Zara product data.\\n- **Web Data Etsy Products**: Quickly read structured Etsy product data.\\n- **Web Data Bestbuy Products**: Quickly read structured Best Buy product data.'},\n",
       " {'filename': 'brightdata-mcp-main/assets/Tools.md',\n",
       "  'section': '## Social Media Data Extraction\\n\\nThe following features are available for extracting data from social media platforms:\\n\\n- **Web Data LinkedIn People Profile**: Quickly read structured LinkedIn people profile data.\\n- **Web Data LinkedIn Company Profile**: Quickly read structured LinkedIn company profile data.\\n- **Web Data LinkedIn Job Listings**: Quickly read structured LinkedIn job listings data.\\n- **Web Data LinkedIn Posts**: Quickly read structured LinkedIn posts data.\\n- **Web Data LinkedIn People Search**: Quickly read structured LinkedIn people search data.\\n- **Web Data Instagram Profiles**: Quickly read structured Instagram profile data.\\n- **Web Data Instagram Posts**: Quickly read structured Instagram post data.\\n- **Web Data Instagram Reels**: Quickly read structured Instagram reel data.\\n- **Web Data Instagram Comments**: Quickly read structured Instagram comments data.\\n- **Web Data Facebook Posts**: Quickly read structured Facebook post data.\\n- **Web Data Facebook Marketplace Listings**: Quickly read structured Facebook marketplace listing data.\\n- **Web Data Facebook Company Reviews**: Quickly read structured Facebook company reviews data.\\n- **Web Data Tiktok Profiles**: Quickly read structured TikTok profiles data.\\n- **Web Data Tiktok Posts**: Quickly read structured TikTok post data.\\n- **Web Data Tiktok Shop**: Quickly read structured TikTok shop data.\\n- **Web Data Tiktok Comments**: Quickly read structured TikTok comments data.'},\n",
       " {'filename': 'brightdata-mcp-main/assets/Tools.md',\n",
       "  'section': '## Data Extraction from Online Directories\\n\\nThe following features are available for extracting data from online directories:\\n\\n- **Web Data Zoominfo Company Profile**: Quickly read structured ZoomInfo company profile data.\\n- **Web Data Crunchbase Company**: Quickly read structured Crunchbase company data.\\n- **Web Data Google Maps Reviews**: Quickly read structured Google Maps reviews data.'},\n",
       " {'filename': 'brightdata-mcp-main/assets/Tools.md',\n",
       "  'section': '## Data Extraction from Online Marketplaces\\n\\nThe following features are available for extracting data from online marketplaces:\\n\\n- **Web Data Etsy Products**: Quickly read structured Etsy product data.\\n- **Web Data eBay Product**: Quickly read structured eBay product data.\\n- **Web Data Walmart Product**: Quickly read structured Walmart product data.'},\n",
       " {'filename': 'brightdata-mcp-main/assets/Tools.md',\n",
       "  'section': '## Data Extraction from Online Shopping Platforms\\n\\nThe following features are available for extracting data from online shopping platforms:\\n\\n- **Web Data Google Shopping**: Quickly read structured Google Shopping data.\\n- **Web Data Google Play Store**: Quickly read structured Google Play Store data.\\n- **Web Data Apple App Store**: Quickly read structured Apple App Store data.'},\n",
       " {'filename': 'brightdata-mcp-main/assets/Tools.md',\n",
       "  'section': '## Data Extraction from News and Finance\\n\\nThe following features are available for extracting data from news and finance:\\n\\n- **Web Data Reuter News**: Quickly read structured Reuter news data.\\n- **Web Data Yahoo Finance Business**: Quickly read structured Yahoo Finance business data.'},\n",
       " {'filename': 'brightdata-mcp-main/assets/Tools.md',\n",
       "  'section': '## Data Extraction from Other Platforms\\n\\nThe following features are available for extracting data from other platforms:\\n\\n- **Web Data Youtube Videos**: Quickly read structured YouTube videos data.\\n- **Web Data Youtube Profiles**: Quickly read structured YouTube profiles data.\\n- **Web Data Youtube Comments**: Quickly read structured YouTube comments data.\\n- **Web Data Reddit Posts**: Quickly read structured Reddit posts data.'},\n",
       " {'filename': 'brightdata-mcp-main/examples/README.md',\n",
       "  'section': \"## Introduction to MCP Usage Examples\\n\\nA curated list of community demos using Bright Data's MCP server. This document showcases practical examples of how the MCP server can be utilized to create innovative applications and workflows.\"},\n",
       " {'filename': 'brightdata-mcp-main/examples/README.md',\n",
       "  'section': \"## üß† Notable Examples\\n\\n- **AI voice agent that closed 4 deals & made $596 overnight ü§ë**  \\n  [üìπ YouTube Demo](https://www.youtube.com/watch?v=YGzT3sVdwdY) \\n  [üíª GitHub Repo](https://github.com/llSourcell/my_ai_intern)\\n\\n- **Langgraph with mcp-adapters demo**\\n\\n  [üìπ YouTube Demo](https://www.youtube.com/watch?v=6DXuadyaJ4g)\\n  \\n  [üíª Source Code](https://github.com/techwithtim/BrightDataMCPServerAgent)\\n\\n- **Researcher Agent built with Google ADK that is connected to Bright Data's MCP to fetch real-time data**\\n\\n   [üìπ YouTube Demo](https://www.youtube.com/watch?v=r7WG6dXWdUI)\\n  \\n  [üíªSource Code](https://github.com/MeirKaD/MCP_ADK)\"},\n",
       " {'filename': 'brightdata-mcp-main/examples/README.md',\n",
       "  'section': '## Efficient MCP Usage\\n\\n- **Replacing 3 MCP servers with our MCP server to avoid getting blocked ü§Ø**  \\n  [üìπ YouTube Demo](https://www.youtube.com/watch?v=0xmE0OJrNmg)'},\n",
       " {'filename': 'brightdata-mcp-main/examples/README.md',\n",
       "  'section': '## Advanced Scrape and AI Use Cases\\n\\n- **Scrape ANY Website In Realtime With This Powerful AI MCP Server**\\n\\n   [üìπ YouTube Demo](https://www.youtube.com/watch?v=bL5JIeGL3J0)'},\n",
       " {'filename': 'brightdata-mcp-main/examples/README.md',\n",
       "  'section': '## Multi-Agent Job and Task Automation\\n\\n- **Multi-Agent job finder using Bright Data MCP and TypeScript from SCRATCH**\\n\\n   [üìπ YouTube Demo](https://www.youtube.com/watch?v=45OtteCGFiI)\\n   \\n   [üíªSource Code](https://github.com/bitswired/jobwizard)'},\n",
       " {'filename': 'brightdata-mcp-main/examples/README.md',\n",
       "  'section': '## Tutorials and CLI Demos\\n\\n- **Usage example with Gemini CLI**\\n\\n    [üìπ YouTube Tutorial](https://www.youtube.com/watch?v=FE1LChbgFEw)'},\n",
       " {'filename': 'brightdata-mcp-main/examples/README.md',\n",
       "  'section': '## Contributing Examples\\n\\nGot a cool example? Open a PR or contact us!'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9d4d98f-7b6f-4472-97c1-ebe7daaed329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def save_chunks_to_csv(data, filename='evidently_chunks.csv'):\n",
    "    if not data:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "\n",
    "    # Get all unique keys across all dictionaries\n",
    "    fieldnames = set()\n",
    "    for row in data:\n",
    "        fieldnames.update(row.keys())\n",
    "    fieldnames = list(fieldnames)\n",
    "\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "    print(f\"Saved {len(data)} chunks to {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e19c9a69-a587-4162-ade4-8e207e43e4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 48 chunks to evidently_chunks.csv\n"
     ]
    }
   ],
   "source": [
    "save_chunks_to_csv(evidently_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83860cec-ce23-4fd6-96a0-19691ed9ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunks_to_markdown(data, filename='evidently_chunks.md'):\n",
    "    if not data:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for chunk in data:\n",
    "            section_title = chunk.get(\"title\", \"Untitled Section\")\n",
    "            section_content = chunk.get(\"content\", \"\")\n",
    "\n",
    "            f.write(f\"## {section_title}\\n\\n\")\n",
    "            f.write(f\"{section_content.strip()}\\n\\n\")\n",
    "            f.write(\"---\\n\\n\")\n",
    "\n",
    "    print(f\"Saved {len(data)} sections to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf6a9332-790b-470a-9bbd-06542c5f7566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 48 sections to evidently_chunks.md\n"
     ]
    }
   ],
   "source": [
    "save_chunks_to_markdown(evidently_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "327b2adb-8a7f-4c7f-b5dc-954d99c0e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chunks_to_markdown(data, filename='evidently_chunks2.md'):\n",
    "    if not data:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for chunk in data:\n",
    "            #section_title = chunk.get(\"title\", \"Untitled Section\")  # Optional: you can skip this if titles aren't available\n",
    "            section_content = chunk.get(\"section\", \"\").strip()\n",
    "\n",
    "            #f.write(f\"## {section_title}\\n\\n\")\n",
    "            f.write(f\"{section_content}\\n\\n\")\n",
    "            f.write(\"---\\n\\n\")\n",
    "\n",
    "    print(f\"Saved {len(data)} sections to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d672bfb-0fd7-4097-986f-6cdb7a2a3f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 48 sections to evidently_chunks2.md\n"
     ]
    }
   ],
   "source": [
    "save_chunks_to_markdown(evidently_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ab98fa-d6d5-4957-80a7-b4e0955e08b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
